{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9300838e107a5e61",
   "metadata": {},
   "source": [
    "Célula para importação"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from pyspark.sql.functions import sum, avg, count, when, col, std, format_number, year, month, day, desc, asc, cast,mean, expr\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, TimestampType, FloatType, LongType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d605e850ba0c417",
   "metadata": {},
   "source": [
    "Célula para pegar o caminho raiz do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "id": "44aa65831967ce65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:12:07.638161Z",
     "start_time": "2024-06-17T18:12:07.625982Z"
    }
   },
   "source": [
    "#Pega a pasta atual em que o arquivo se encontra\n",
    "RAIZ: str = os.getcwd()\n",
    "#Junta o caminho raiz com o arquivo desejado\n",
    "BASE_DIR: str = os.path.join(RAIZ, 'Retail_Transaction.xlsx')\n",
    "#Importa o arquivo, como é CSV é necessário separar por vírgula"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "614259266c666cd6",
   "metadata": {},
   "source": [
    "Inicialização do Pyspark junto com a importação do arquivo com pandas para mais na frente converter para spark"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c2e0b2adce78d09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:13:05.286540Z",
     "start_time": "2024-06-17T18:12:09.300518Z"
    }
   },
   "source": [
    "# Criar uma SparkSession\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .appName(\"Consumo\") \\\n",
    "    .getOrCreate()\n",
    "# Ler dados de um arquivo excel\n",
    "df: pd.DataFrame  = pd.read_excel(BASE_DIR, sheet_name='Sheet1')"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "8134ba1cdf4e4445",
   "metadata": {},
   "source": [
    "Formatação prévia das datas devido a problemas que podem acontecer no futuro "
   ]
  },
  {
   "cell_type": "code",
   "id": "cccdde64d026b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:13:17.860273Z",
     "start_time": "2024-06-17T18:13:16.258215Z"
    }
   },
   "source": [
    "# transforma o dataframe em uma lista devido a problemas comimpor formatação\n",
    "transacao_datas: list[str] | list[list[str]] = list(df['TransactionDate'])\n",
    "# separa os valores das datas\n",
    "for index, datas in enumerate(transacao_datas):\n",
    "    transacao_datas[index]: list[str] | str = datas.split(\"/\")\n",
    "# caso a data ou mês só tenha 1 digito, adiciona um 0 na frfente\n",
    "for index, datas in enumerate(transacao_datas):\n",
    "    for internal_index, valor in enumerate(datas):\n",
    "        if len(valor) < 2:\n",
    "            transacao_datas[index][internal_index] : str = f\"0{valor}\"\n",
    "# junta as datas\n",
    "for index, datas in enumerate(transacao_datas):\n",
    "    transacao_datas[index]: list[str] | str = \"/\".join(datas)\n",
    "# transforma as datas em uma coluna dataframe\n",
    "df_datas = pd.DataFrame({\"TransactionDate\": transacao_datas})\n",
    "# atribui as datas a coluna do datrame\n",
    "df['TransactionDate'] = df_datas\n",
    "# converte as datas pra datetime\n",
    "df['TransactionDate'] = pd.to_datetime(df.TransactionDate)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "351e72daa0fa56ee",
   "metadata": {},
   "source": [
    "Inspeção Inicial de dados"
   ]
  },
  {
   "cell_type": "code",
   "id": "f854bdd443f92f49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:13:53.381912Z",
     "start_time": "2024-06-17T18:13:32.551439Z"
    }
   },
   "source": [
    "# Converte um dataframe pandas para um dataframe spark\n",
    "Transaction: DataFrame = spark.createDataFrame(df)\n",
    "# Mostrar os primeiros registros do DataFrame\n",
    "Transaction.show(5)\n",
    "\n",
    "# Mostrar o esquema do DataFrame\n",
    "Transaction.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "|Unnamed: 0|CustomerID|ProductID|Quantity|      Price|    TransactionDate|PaymentMethod|       StoreLocation|ProductCategory|DiscountApplied(%)|TotalAmount|\n",
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "|         0|    109318|        C|       7|80.07984415|2023-12-26 12:32:00|         Cash|176 Andrew Cliffs...|          Books|        18.6770995|455.8627638|\n",
      "|         1|    993229|        C|       4|75.19522942|2023-08-05 00:00:00|         Cash|11635 William Wel...|     Home Decor|       14.12136502|258.3065464|\n",
      "|         2|    579675|        A|       8|31.52881648|2024-03-11 18:51:00|         Cash|910 Mendez Ville ...|          Books|       15.94370066|212.0156509|\n",
      "|         3|    799826|        D|       5|98.88021828|2023-10-27 22:00:00|       PayPal|87522 Sharon Corn...|          Books|        6.68633678|461.3437694|\n",
      "|         4|    121413|        A|       7|93.18851246|2023-12-22 11:38:00|         Cash|0070 Michelle Isl...|    Electronics|       4.030095691|626.0304837|\n",
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- ProductID: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- TransactionDate: timestamp (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- StoreLocation: string (nullable = true)\n",
      " |-- ProductCategory: string (nullable = true)\n",
      " |-- DiscountApplied(%): double (nullable = true)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "606f60840287ac1b",
   "metadata": {},
   "source": [
    "Limpeza e Tratamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fb85581943fbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "5264c99783770afb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:15:14.394425Z",
     "start_time": "2024-06-17T18:15:02.662577Z"
    }
   },
   "source": [
    "# Contar valores nulos por coluna\n",
    "Transaction.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# Remover linhas com qualquer valor nulo\n",
    "Transaction_Clean: DataFrame = Transaction.dropna()\n",
    "# Remove duplicados\n",
    "Transaction_Clean: DataFrame = Transaction_Clean.dropDuplicates()\n",
    "# Filtrar linhas com quantidade ou preço negativos\n",
    "Transaction_Clean: DataFrame = Transaction_Clean.filter((Transaction_Clean[\"Quantity\"] > 0) & (Transaction_Clean[\"Price\"] > 0) & (Transaction_Clean[\"DiscountApplied(%)\"] >= 0))\n",
    "# formata as colunas que possuem números reais para possuir no máximo 2 casas decimais e também converte o tipo das determinadas columas para número real\n",
    "Transaction_Clean: DataFrame = (Transaction_Clean.withColumn(\"Price\", col(\"Price\").cast(\"float\")).withColumn(\"Price\", format_number(Transaction_Clean[\"Price\"], 2))\n",
    "                                .withColumn(\"DiscountApplied(%)\", col(\"DiscountApplied(%)\").cast(\"float\")).withColumn(\"DiscountApplied(%)\", format_number(Transaction_Clean[\"DiscountApplied(%)\"], 2))\n",
    "                                .withColumn(\"TotalAmount\", col(\"TotalAmount\").cast(\"float\")).withColumn(\"TotalAmount\", format_number(Transaction_Clean[\"TotalAmount\"], 2)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+--------+-----+---------------+-------------+-------------+---------------+------------------+-----------+\n",
      "|Unnamed: 0|CustomerID|ProductID|Quantity|Price|TransactionDate|PaymentMethod|StoreLocation|ProductCategory|DiscountApplied(%)|TotalAmount|\n",
      "+----------+----------+---------+--------+-----+---------------+-------------+-------------+---------------+------------------+-----------+\n",
      "|         0|         0|        0|       0|    0|              0|            0|            0|              0|                 0|          0|\n",
      "+----------+----------+---------+--------+-----+---------------+-------------+-------------+---------------+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[MISSING_ATTRIBUTES.RESOLVED_ATTRIBUTE_APPEAR_IN_OPERATION] Resolved attribute(s) \"Price\" missing from \"Unnamed: 0\", \"CustomerID\", \"ProductID\", \"Quantity\", \"Price\", \"TransactionDate\", \"PaymentMethod\", \"StoreLocation\", \"ProductCategory\", \"DiscountApplied(%)\", \"TotalAmount\" in operator !Project [Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, format_number(Price#4, 2) AS Price#323, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10]. Attribute(s) with the same name appear in the operation: \"Price\".\nPlease check if the right attribute(s) are used.;\n!Project [Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, format_number(Price#4, 2) AS Price#323, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10]\n+- Project [Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, cast(Price#4 as float) AS Price#311, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10]\n   +- Filter (((Quantity#3L > cast(0 as bigint)) AND (Price#4 > cast(0 as double))) AND (DiscountApplied(%)#9 >= cast(0 as double)))\n      +- Deduplicate [Quantity#3L, TotalAmount#10, CustomerID#1L, Price#4, StoreLocation#7, ProductCategory#8, Unnamed: 0#0L, TransactionDate#5, ProductID#2, PaymentMethod#6, DiscountApplied(%)#9]\n         +- Filter atleastnnonnulls(11, Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, Price#4, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10)\n            +- LogicalRDD [Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, Price#4, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10], false\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m Transaction_Clean: DataFrame \u001B[38;5;241m=\u001B[39m Transaction_Clean\u001B[38;5;241m.\u001B[39mfilter((Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuantity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m&\u001B[39m (Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m&\u001B[39m (Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# formata as colunas que possuem números reais para possuir no máximo 2 casas decimais e também converte o tipo das determinadas columas para número real\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m Transaction_Clean: DataFrame \u001B[38;5;241m=\u001B[39m (\u001B[43mTransaction_Clean\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfloat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTransaction_Clean\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m                                 \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m, format_number(Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m     12\u001B[0m                                 \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m, format_number(Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m2\u001B[39m)))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5174\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[1;34m(self, colName, col)\u001B[0m\n\u001B[0;32m   5169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[0;32m   5170\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[0;32m   5171\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   5172\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[0;32m   5173\u001B[0m     )\n\u001B[1;32m-> 5174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: [MISSING_ATTRIBUTES.RESOLVED_ATTRIBUTE_APPEAR_IN_OPERATION] Resolved attribute(s) \"Price\" missing from \"Unnamed: 0\", \"CustomerID\", \"ProductID\", \"Quantity\", \"Price\", \"TransactionDate\", \"PaymentMethod\", \"StoreLocation\", \"ProductCategory\", \"DiscountApplied(%)\", \"TotalAmount\" in operator !Project [Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, format_number(Price#4, 2) AS Price#323, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10]. Attribute(s) with the same name appear in the operation: \"Price\".\nPlease check if the right attribute(s) are used.;\n!Project [Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, format_number(Price#4, 2) AS Price#323, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10]\n+- Project [Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, cast(Price#4 as float) AS Price#311, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10]\n   +- Filter (((Quantity#3L > cast(0 as bigint)) AND (Price#4 > cast(0 as double))) AND (DiscountApplied(%)#9 >= cast(0 as double)))\n      +- Deduplicate [Quantity#3L, TotalAmount#10, CustomerID#1L, Price#4, StoreLocation#7, ProductCategory#8, Unnamed: 0#0L, TransactionDate#5, ProductID#2, PaymentMethod#6, DiscountApplied(%)#9]\n         +- Filter atleastnnonnulls(11, Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, Price#4, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10)\n            +- LogicalRDD [Unnamed: 0#0L, CustomerID#1L, ProductID#2, Quantity#3L, Price#4, TransactionDate#5, PaymentMethod#6, StoreLocation#7, ProductCategory#8, DiscountApplied(%)#9, TotalAmount#10], false\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "cc888be1df786911",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:15:37.571870Z",
     "start_time": "2024-06-17T18:15:21.687589Z"
    }
   },
   "source": [
    "Transaction_Clean.show()\n",
    "Transaction_Clean.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "|Unnamed: 0|CustomerID|ProductID|Quantity|      Price|    TransactionDate|PaymentMethod|       StoreLocation|ProductCategory|DiscountApplied(%)|TotalAmount|\n",
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "|       517|    768027|        D|       1|81.75612284|2023-07-26 21:59:00|         Cash|644 Patel River\\n...|          Books|       15.86988478|68.78152035|\n",
      "|       535|    608285|        B|       6|59.68429524|2023-09-29 06:46:00|  Credit Card|1002 Dawn Plain A...|    Electronics|        7.63302201|330.7714791|\n",
      "|      1163|    744100|        A|       1|31.94335812|2024-03-11 16:00:00|       PayPal|23849 Adriana Lod...|     Home Decor|       16.59913828|26.64103593|\n",
      "|      1324|    459092|        D|       7| 10.0488432|2023-07-05 18:04:00|   Debit Card|8936 Tracey Crest...|    Electronics|       5.430980505|66.52164741|\n",
      "|      1866|    768914|        B|       1|60.66176211|2024-02-25 15:29:00|   Debit Card|89340 Griffin Gar...|       Clothing|       11.99124837|53.38765955|\n",
      "|      1999|    656206|        B|       3|41.47623451|2024-01-23 05:26:00|       PayPal|230 Julia Express...|     Home Decor|       5.061150274|118.1311798|\n",
      "|      2336|    963494|        D|       2|59.07908638|2024-02-01 12:55:00|   Debit Card|346 Gibson Avenue...|     Home Decor|       14.98347397|100.4539737|\n",
      "|      2392|    447546|        D|       8| 37.3279794|2023-10-20 11:34:00|  Credit Card|5945 Glenn Dale\\n...|     Home Decor|       4.126505121|286.3011073|\n",
      "|      2616|    638779|        D|       8|21.00773696|2023-12-19 23:48:00|         Cash|2240 Hanna Ridges...|          Books|       19.63173088|135.0684366|\n",
      "|      2617|    781426|        B|       3|93.44161311|2024-02-08 01:40:00|   Debit Card|59306 Walker Radi...|     Home Decor|       11.87070264|247.0483112|\n",
      "|      2743|    546141|        A|       3|29.10985957|2023-08-15 04:03:00|   Debit Card|8772 Daniel Prair...|          Books|       18.58589216|71.09859738|\n",
      "|      2747|     93627|        A|       5|82.34418964|2023-11-07 07:55:00|  Credit Card|238 Reynolds Fort...|          Books|       17.16716378|341.0401387|\n",
      "|      2877|    920184|        B|       2|34.76862288|2024-01-16 11:46:00|         Cash|04673 Austin Comm...|          Books|       4.647131416|66.30575856|\n",
      "|      3447|    469020|        C|       4| 80.0384907|2023-12-21 14:53:00|   Debit Card|370 Daniel Center...|    Electronics|       4.655064371|305.2505898|\n",
      "|      3474|    108849|        C|       7|31.77342219|2023-12-12 16:59:00|   Debit Card|671 Robin Streets...|          Books|       8.875925506|202.6726583|\n",
      "|      3475|    401637|        C|       7| 93.2135735|2023-05-18 10:11:00|       PayPal|29985 James Meado...|    Electronics|       16.27085357|546.3285061|\n",
      "|      3567|    221447|        A|       9|77.15103312|2023-12-27 11:37:00|  Credit Card|8782 Lewis Club A...|       Clothing|       9.041211451|631.5808057|\n",
      "|      3879|    567549|        A|       5|41.50708416|2023-05-25 10:29:00|         Cash|9834 Jacqueline N...|          Books|       1.073318953|205.3079038|\n",
      "|      3895|    156510|        A|       3|76.19004044|2023-05-10 08:19:00|       PayPal|24081 Andre Hill ...|       Clothing|       11.94946545|201.2572137|\n",
      "|      4024|    588166|        C|       8|59.72164806|2023-08-18 21:04:00|   Debit Card|926 Swanson Falls...|          Books|        6.40080736|447.1918433|\n",
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- ProductID: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- TransactionDate: timestamp (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- StoreLocation: string (nullable = true)\n",
      " |-- ProductCategory: string (nullable = true)\n",
      " |-- DiscountApplied(%): double (nullable = true)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "b77c67942849e3b1",
   "metadata": {},
   "source": [
    "Calcular o total de vendas por categoria de produto"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5fd1780af8937aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:15:52.903504Z",
     "start_time": "2024-06-17T18:15:37.576420Z"
    }
   },
   "source": [
    "Transaction_Agrupado: DataFrame = Transaction_Clean.groupBy(\"ProductCategory\").agg(sum(\"TotalAmount\").alias(\"TotalSales\"))\n",
    "Transaction_Agrupado.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|ProductCategory|       TotalSales|\n",
      "+---------------+-----------------+\n",
      "|    Electronics|6196734.758744199|\n",
      "|       Clothing|6205502.348754664|\n",
      "|          Books|6257837.043115519|\n",
      "|     Home Decor|6173421.355771133|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "e804fd4c3f980590",
   "metadata": {},
   "source": [
    "Calcular a média de descontos aplicados por método de pagamento"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f577274cd4fd06c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:16:07.932757Z",
     "start_time": "2024-06-17T18:15:52.907520Z"
    }
   },
   "source": [
    "Transaction_Descontos = Transaction_Clean.groupBy(\"PaymentMethod\").agg(avg(\"DiscountApplied(%)\").alias(\"AverageDiscount\"))\n",
    "Transaction_Descontos.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|PaymentMethod|   AverageDiscount|\n",
      "+-------------+------------------+\n",
      "|  Credit Card| 10.03116308028181|\n",
      "|       PayPal|10.008327592227264|\n",
      "|         Cash| 10.03301790466662|\n",
      "|   Debit Card|10.008069497067055|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "aae950e2f5f624b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:16:25.647844Z",
     "start_time": "2024-06-17T18:16:07.941331Z"
    }
   },
   "source": [
    "# Adicionar colunas de ano e mês\n",
    "Transaction_Clean = Transaction_Clean.withColumn(\"Year\", year(\"TransactionDate\"))\n",
    "Transaction_Clean = Transaction_Clean.withColumn(\"Month\", month(\"TransactionDate\"))\n",
    "Transaction_Clean = Transaction_Clean.withColumn(\"Day\", day(\"TransactionDate\"))\n",
    "# Calcular vendas mensais\n",
    "Transaction_venda_diaria = Transaction_Clean.groupBy(\"Month\", \"Day\").agg(sum(\"TotalAmount\").alias(\"DailySales\")).orderBy(desc(\"DailySales\"))\n",
    "Transaction_venda_diaria.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------------+\n",
      "|Month|Day|       DailySales|\n",
      "+-----+---+-----------------+\n",
      "|   12|  1|   83049.51594861|\n",
      "|    1| 13|82985.01451571201|\n",
      "|   10|  3|81782.03127321001|\n",
      "|    6| 29|   81664.26111011|\n",
      "|    1| 30|   80351.40598898|\n",
      "|    6|  5|80277.38514937699|\n",
      "|    7| 18|79690.51131472999|\n",
      "|    4|  4|   79028.52027835|\n",
      "|    1| 16|   78970.38160429|\n",
      "|   11|  2|   78764.65648014|\n",
      "|    7| 31|   78526.23379413|\n",
      "|    9| 28|   78272.38116167|\n",
      "|    3| 15|  77828.184627962|\n",
      "|   11| 16|  77583.698018286|\n",
      "|    3|  5|77546.98296815001|\n",
      "|    9| 22|   77512.96266329|\n",
      "|    8| 24|   77381.52466296|\n",
      "|    3| 25|77144.85874866002|\n",
      "|    7|  8|  77083.971073889|\n",
      "|    5| 25|76727.28133312998|\n",
      "+-----+---+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "ce84ea29891bd100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:16:46.028622Z",
     "start_time": "2024-06-17T18:16:25.655879Z"
    }
   },
   "source": [
    "Transaction_venda_mensal = Transaction_Clean.groupBy(\"Year\", \"Month\").agg(sum(\"TotalAmount\").alias(\"MonthlySales\")).orderBy(desc(\"MonthlySales\"))\n",
    "Transaction_venda_mensal.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|Year|Month|      MonthlySales|\n",
      "+----+-----+------------------+\n",
      "|2023|    7| 2132550.517749446|\n",
      "|2024|    1|2128345.2769411867|\n",
      "|2023|   12|2125651.0656809583|\n",
      "|2023|    8| 2109352.646045025|\n",
      "|2024|    3| 2108248.042126509|\n",
      "|2023|    5|2099576.1008348833|\n",
      "|2023|    6|2066364.8233160693|\n",
      "|2023|   11| 2051277.297132632|\n",
      "|2023|    9|2050334.6010149356|\n",
      "|2023|   10|2049450.6981157311|\n",
      "|2024|    2|1973154.1172272428|\n",
      "|2024|    4|1878349.5323468787|\n",
      "|2023|    4|   60840.787854022|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "d21e2cb0942420a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:16:46.130065Z",
     "start_time": "2024-06-17T18:16:46.032640Z"
    }
   },
   "source": [
    "numeric_cols: list = []\n",
    "categorical_cols: list = []\n",
    "\n",
    "Transaction_schema = Transaction_Clean.schema\n",
    "# Separar colunas com base no tipo de dado\n",
    "for field in Transaction_schema:\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType, LongType)):\n",
    "        numeric_cols.append(field.name)\n",
    "    elif isinstance(field.dataType, (StringType, TimestampType)):\n",
    "        categorical_cols.append(field.name)\n",
    "\n",
    "numeric_cols.remove('Unnamed: 0')\n",
    "numeric_cols.remove(\"CustomerID\")\n",
    "Transaction_numerico = Transaction_Clean.select(numeric_cols)\n",
    "Transaction_categorico = Transaction_Clean.select(categorical_cols)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "41d7159e19c4e602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:20:15.219711Z",
     "start_time": "2024-06-17T18:16:46.133080Z"
    }
   },
   "source": [
    "def calcula_outlier(dataframe: DataFrame, col_name: str) -> tuple:\n",
    "    q1 = dataframe.approxQuantile(col_name, [0.25], 0.01)[0]\n",
    "    q3 = dataframe.approxQuantile(col_name, [0.75], 0.01)[0]\n",
    "    iqr = q3 - q1\n",
    "    outliers_inferior = q1 - 1.5 * iqr\n",
    "    outliers_superior = q3 + 1.5 * iqr\n",
    "    return outliers_inferior, outliers_superior\n",
    "\n",
    "# Calcular os limites para cada coluna numérica\n",
    "outliers_limite = {col_name: calcula_outlier(Transaction_numerico, col_name) for col_name in numeric_cols}\n",
    "print(outliers_limite)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Quantity': (-3.0, 13.0), 'Price': (-34.145917880000006, 143.66259860000002), 'DiscountApplied(%)': (-9.8155356225, 29.6791015975), 'TotalAmount': (-299.96853877499996, 750.2632981449999), 'Year': (2021.5, 2025.5), 'Month': (-6.0, 18.0), 'Day': (-14.5, 45.5)}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "62113fdb6f52e474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:21:38.398596Z",
     "start_time": "2024-06-17T18:20:15.221719Z"
    }
   },
   "source": [
    "# Função para identificar outliers\n",
    "def identifica_outlier(dataframe: DataFrame, col_name: str, outliers_inferiores: float, outliers_superiores: float) -> DataFrame:\n",
    "    return dataframe.filter((col(col_name) < outliers_inferiores) | (col(col_name) > outliers_superiores))\n",
    "\n",
    "# Identificar e mostrar os outliers\n",
    "outliers = {}\n",
    "for col_name, (outlier_inferior, outlier_superior) in outliers_limite.items():\n",
    "    transaction_outlier = identifica_outlier(Transaction_numerico, col_name, outlier_inferior, outlier_superior)\n",
    "    outliers[col_name] = transaction_outlier\n",
    "\n",
    "# Mostrar os outliers para cada coluna\n",
    "for col_name, transaction_outlier in outliers.items():\n",
    "    print(f\"Outliers na coluna {col_name}:\")\n",
    "    transaction_outlier.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers na coluna Quantity:\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|Year|Month|Day|\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "\n",
      "Outliers na coluna Price:\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|Year|Month|Day|\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "\n",
      "Outliers na coluna DiscountApplied(%):\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|Year|Month|Day|\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "\n",
      "Outliers na coluna TotalAmount:\n",
      "+--------+-----------+------------------+-----------+----+-----+---+\n",
      "|Quantity|      Price|DiscountApplied(%)|TotalAmount|Year|Month|Day|\n",
      "+--------+-----------+------------------+-----------+----+-----+---+\n",
      "|       9| 90.3351513|       5.341062648|769.5926485|2024|    3|  1|\n",
      "|       9|96.09278818|       3.435480217|835.1238551|2023|    9| 15|\n",
      "|       9| 98.7028445|       1.266869772| 877.071672|2024|    1|  2|\n",
      "|       9|90.36379105|       1.267585838|802.9651719|2023|    5| 10|\n",
      "|       9|96.08480283|       13.16072454|750.9541195|2023|    8| 16|\n",
      "|       9|95.59223377|       8.070660116|790.8957854|2023|    8|  1|\n",
      "|       9|98.09163918|       11.33571314|782.7502711|2023|   11| 24|\n",
      "|       9|96.94298506|       3.331673288|843.4184537|2024|    3|  1|\n",
      "|       9|94.66163024|       2.077904086|834.2518712|2024|    2|  1|\n",
      "|       9|91.16090501|       0.262245556|818.2965563|2024|    2|  8|\n",
      "|       9|99.88844086|       1.850444087|  882.36055|2024|    3| 28|\n",
      "|       9|90.10054469|       5.956831588|762.6006628|2023|   11| 18|\n",
      "|       9|94.77635223|       7.220298815|791.3989475|2023|    7| 10|\n",
      "|       8|98.95536366|       2.886172202|768.7947317|2023|    5|  3|\n",
      "|       9|85.34599674|       0.524163412|764.0877982|2023|    7| 22|\n",
      "|       9|93.76589606|       2.461751586|823.1185137|2024|    3| 26|\n",
      "|       9|98.40126988|       13.94404586|762.1213651|2023|    5| 26|\n",
      "|       9|98.07391721|       13.59411485|762.6747264|2023|    6| 28|\n",
      "|       9|90.92989482|       0.579356056|813.6277827|2023|   12| 29|\n",
      "|       9|98.83947846|       5.940162007|836.7142798|2023|   11| 19|\n",
      "+--------+-----------+------------------+-----------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Outliers na coluna Year:\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|Year|Month|Day|\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "\n",
      "Outliers na coluna Month:\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|Year|Month|Day|\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "\n",
      "Outliers na coluna Day:\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|Year|Month|Day|\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "+--------+-----+------------------+-----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "bbb4ba0335718fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:33:14.475311Z",
     "start_time": "2024-06-17T18:33:14.470615Z"
    }
   },
   "source": [
    "os.makedirs(os.path.join(RAIZ,\"Relatórios\"), exist_ok=True)\n",
    "CAMINHO_PARA_SALVAR = os.path.join(RAIZ, \"Relatórios\")\n",
    "os.environ['HADOOP_HOME'] = '/usr/local/hadoop-3'"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "9a44da722311d81a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:33:49.977607Z",
     "start_time": "2024-06-17T18:33:49.977607Z"
    }
   },
   "source": [
    "Transaction_Agrupado.write.csv(os.path.join(CAMINHO_PARA_SALVAR, \"TotalSalesPorCategoria.csv\"), header=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a326267191f66b5c",
   "metadata": {},
   "source": [
    "Transaction_Descontos.write.csv(os.path.join(CAMINHO_PARA_SALVAR, \"AverageDiscountPorMetodoDePagamento.csv\"), header=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eaddbd60d4fde520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:33:49.974580Z",
     "start_time": "2024-06-17T18:33:31.248417Z"
    }
   },
   "source": [
    "Transaction_venda_diaria.write.csv(os.path.join(CAMINHO_PARA_SALVAR, \"DailySales.csv\"), header=True)"
   ],
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o854.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mTransaction_venda_diaria\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCAMINHO_PARA_SALVAR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDailySales.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[0;32m   1847\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[0;32m   1848\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1862\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[0;32m   1863\u001B[0m )\n\u001B[1;32m-> 1864\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o854.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "e0cb6afc9aa6b89d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:25:54.275613Z",
     "start_time": "2024-06-17T18:25:54.272611Z"
    }
   },
   "source": [
    "Transaction_venda_mensal.write.csv(os.path.join(CAMINHO_PARA_SALVAR, \"MonthlySales.csv\"), header=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "286564612afe781b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:25:54.287745Z",
     "start_time": "2024-06-17T18:25:54.286727Z"
    }
   },
   "source": [
    "outliers['TotalAmount'].write.csv(os.path.join(CAMINHO_PARA_SALVAR, \"OutliersTotal_Amount.csv\"), header=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c25716ea81e266d7",
   "metadata": {},
   "source": [
    "spark.stop()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
