{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Célula para importação",
   "id": "9300838e107a5e61"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-16T22:31:02.868650Z",
     "start_time": "2024-06-16T22:31:02.784165Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import sum, avg, count, when, col, std, format_number, year, month, day, desc, asc, cast,mean, expr\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, TimestampType, FloatType, LongType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Célula para pegar o caminho raiz do arquivo",
   "id": "d605e850ba0c417"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T22:31:06.125918Z",
     "start_time": "2024-06-16T22:31:06.093284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Pega a pasta atual em que o arquivo se encontra\n",
    "RAIZ: str = os.getcwd()\n",
    "#Junta o caminho raiz com o arquivo desejado\n",
    "BASE_DIR: str = os.path.join(RAIZ, 'Retail_Transaction.xlsx')\n",
    "#Importa o arquivo, como é CSV é necessário separar por vírgula"
   ],
   "id": "44aa65831967ce65",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inicialização do Pyspark junto com a importação do arquivo com pandas para mais na frente converter para spark",
   "id": "614259266c666cd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T22:32:54.858912Z",
     "start_time": "2024-06-16T22:31:09.293008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Criar uma SparkSession\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .appName(\"Consumo\") \\\n",
    "    .getOrCreate()\n",
    "# Ler dados de um arquivo excel\n",
    "df: pd.DataFrame  = pd.read_excel(BASE_DIR, sheet_name='Sheet1')"
   ],
   "id": "2c2e0b2adce78d09",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Formatação prévia das datas devido a problemas que podem acontecer no futuro ",
   "id": "8134ba1cdf4e4445"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T22:34:17.525236Z",
     "start_time": "2024-06-16T22:34:16.855251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transforma o dataframe em uma lista devido a problemas com formatação\n",
    "transacao_datas: list[str] | list[list[str]] = list(df['TransactionDate'])\n",
    "# separa os valores das datas\n",
    "for index, datas in enumerate(transacao_datas):\n",
    "    transacao_datas[index]: list[str] | str = datas.split(\"/\")\n",
    "# caso a data ou mês só tenha 1 digito, adiciona um 0 na frfente\n",
    "for index, datas in enumerate(transacao_datas):\n",
    "    for internal_index, valor in enumerate(datas):\n",
    "        if len(valor) < 2:\n",
    "            transacao_datas[index][internal_index] : str = f\"0{valor}\"\n",
    "# junta as datas\n",
    "for index, datas in enumerate(transacao_datas):\n",
    "    transacao_datas[index]: list[str] | str = \"/\".join(datas)\n",
    "# transforma as datas em uma coluna dataframe\n",
    "df_datas = pd.DataFrame({\"TransactionDate\": transacao_datas})\n",
    "# atribui as datas a coluna do datrame\n",
    "df['TransactionDate'] = df_datas\n",
    "# converte as datas pra datetime\n",
    "df['TransactionDate'] = pd.to_datetime(df.TransactionDate)"
   ],
   "id": "cccdde64d026b7",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inspeção Inicial de dados",
   "id": "351e72daa0fa56ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T22:34:48.865295Z",
     "start_time": "2024-06-16T22:34:19.963767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converte um dataframe pandas para um dataframe spark\n",
    "Transaction: DataFrame = spark.createDataFrame(df)\n",
    "# Mostrar os primeiros registros do DataFrame\n",
    "Transaction.show(5)\n",
    "\n",
    "# Mostrar o esquema do DataFrame\n",
    "Transaction.printSchema()"
   ],
   "id": "f854bdd443f92f49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "|Unnamed: 0|CustomerID|ProductID|Quantity|      Price|    TransactionDate|PaymentMethod|       StoreLocation|ProductCategory|DiscountApplied(%)|TotalAmount|\n",
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "|         0|    109318|        C|       7|80.07984415|2023-12-26 12:32:00|         Cash|176 Andrew Cliffs...|          Books|        18.6770995|455.8627638|\n",
      "|         1|    993229|        C|       4|75.19522942|2023-08-05 00:00:00|         Cash|11635 William Wel...|     Home Decor|       14.12136502|258.3065464|\n",
      "|         2|    579675|        A|       8|31.52881648|2024-03-11 18:51:00|         Cash|910 Mendez Ville ...|          Books|       15.94370066|212.0156509|\n",
      "|         3|    799826|        D|       5|98.88021828|2023-10-27 22:00:00|       PayPal|87522 Sharon Corn...|          Books|        6.68633678|461.3437694|\n",
      "|         4|    121413|        A|       7|93.18851246|2023-12-22 11:38:00|         Cash|0070 Michelle Isl...|    Electronics|       4.030095691|626.0304837|\n",
      "+----------+----------+---------+--------+-----------+-------------------+-------------+--------------------+---------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- ProductID: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- TransactionDate: timestamp (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- StoreLocation: string (nullable = true)\n",
      " |-- ProductCategory: string (nullable = true)\n",
      " |-- DiscountApplied(%): double (nullable = true)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Limpeza e Tratamento de dados",
   "id": "606f60840287ac1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2f5fb85581943fbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T22:35:18.907911Z",
     "start_time": "2024-06-16T22:34:59.369505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Contar valores nulos por coluna\n",
    "Transaction.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# Remover linhas com qualquer valor nulo\n",
    "Transaction_Clean: DataFrame = Transaction.dropna()\n",
    "# Remove duplicados\n",
    "Transaction_Clean: DataFrame = Transaction_Clean.dropDuplicates()\n",
    "# Filtrar linhas com quantidade ou preço negativos\n",
    "Transaction_Clean: DataFrame = Transaction_Clean.filter((Transaction_Clean[\"Quantity\"] > 0) & (Transaction_Clean[\"Price\"] > 0) & (Transaction_Clean[\"DiscountApplied(%)\"] >= 0))\n",
    "# formata as colunas que possuem números reais para possuir no máximo 2 casas decimais e também converte o tipo das determinadas columas para número real\n",
    "Transaction_Clean: DataFrame = (Transaction_Clean.withColumn(\"Price\", col(\"Price\").cast(\"float\").withColumn(\"Price\", format_number(Transaction_Clean[\"Price\"], 2)))\n",
    "                                .withColumn(\"DiscountApplied(%)\", col(\"DiscountApplied(%)\").cast(\"float\")).withColumn(\"DiscountApplied(%)\", format_number(Transaction_Clean[\"DiscountApplied(%)\"], 2))\n",
    "                                .withColumn(\"TotalAmount\"), col(\"TotalAmount\").cast(\"float\")).withColumn(\"TotalAmount\", format_number(Transaction_Clean[\"TotalAmount\"], 2))"
   ],
   "id": "5264c99783770afb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+--------+-----+---------------+-------------+-------------+---------------+------------------+-----------+\n",
      "|Unnamed: 0|CustomerID|ProductID|Quantity|Price|TransactionDate|PaymentMethod|StoreLocation|ProductCategory|DiscountApplied(%)|TotalAmount|\n",
      "+----------+----------+---------+--------+-----+---------------+-------------+-------------+---------------+------------------+-----------+\n",
      "|         0|         0|        0|       0|    0|              0|            0|            0|              0|                 0|          0|\n",
      "+----------+----------+---------+--------+-----+---------------+-------------+-------------+---------------+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[164], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m Transaction_Clean: DataFrame \u001B[38;5;241m=\u001B[39m Transaction_Clean\u001B[38;5;241m.\u001B[39mfilter((Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuantity\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m&\u001B[39m (Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m&\u001B[39m (Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# formata as colunas que possuem números reais para possuir no máximo 2 casas decimais e também converte o tipo das determinadas columas para número real\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m Transaction_Clean: DataFrame \u001B[38;5;241m=\u001B[39m (Transaction_Clean\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfloat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTransaction_Clean\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     11\u001B[0m                                 \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m, format_number(Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscountApplied(\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m     12\u001B[0m                                 \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m, format_number(Transaction_Clean[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m2\u001B[39m))\n",
      "\u001B[1;31mTypeError\u001B[0m: 'Column' object is not callable"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Transaction_Clean.show()\n",
    "Transaction_Clean.printSchema()"
   ],
   "id": "cc888be1df786911",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calcular o total de vendas por categoria de produto",
   "id": "b77c67942849e3b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Transaction_Agrupado: DataFrame = Transaction_Clean.groupBy(\"ProductCategory\").agg(sum(\"TotalAmount\").alias(\"TotalSales\"))\n",
    "Transaction_Agrupado.show()"
   ],
   "id": "c5fd1780af8937aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calcular a média de descontos aplicados por método de pagamento",
   "id": "e804fd4c3f980590"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Transaction_Descontos = Transaction_Clean.groupBy(\"PaymentMethod\").agg(avg(\"DiscountApplied(%)\").alias(\"AverageDiscount\"))\n",
    "Transaction_Descontos.show()"
   ],
   "id": "2f577274cd4fd06c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adicionar colunas de ano e mês\n",
    "Transaction_Clean = Transaction_Clean.withColumn(\"Year\", year(\"TransactionDate\"))\n",
    "Transaction_Clean = Transaction_Clean.withColumn(\"Month\", month(\"TransactionDate\"))\n",
    "Transaction_Clean = Transaction_Clean.withColumn(\"Day\", day(\"TransactionDate\"))\n",
    "# Calcular vendas mensais\n",
    "Transaction_venda_diaria = Transaction_Clean.groupBy(\"Month\", \"Day\").agg(sum(\"TotalAmount\").alias(\"DailySales\")).orderBy(desc(\"DailySales\"))\n",
    "Transaction_venda_diaria.show()"
   ],
   "id": "aae950e2f5f624b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:22:46.234333Z",
     "start_time": "2024-06-16T17:22:30.913210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Transaction_venda_mensal = Transaction_Clean.groupBy(\"Year\", \"Month\").agg(sum(\"TotalAmount\").alias(\"MonthlySales\")).orderBy(desc(\"MonthlySales\"))\n",
    "Transaction_venda_mensal.show()"
   ],
   "id": "ce84ea29891bd100",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|Year|Month|      MonthlySales|\n",
      "+----+-----+------------------+\n",
      "|2023|    7|2132550.4899999998|\n",
      "|2024|    1| 2128345.700000001|\n",
      "|2023|   12|2125650.6799999992|\n",
      "|2023|    8|2109352.3199999994|\n",
      "|2024|    3|        2108247.75|\n",
      "|2023|    5|2099576.0999999996|\n",
      "|2023|    6|2066364.4400000002|\n",
      "|2023|   11|        2051277.17|\n",
      "|2023|    9|2050334.5499999998|\n",
      "|2023|   10|2049451.0400000003|\n",
      "|2024|    2|1973154.1399999997|\n",
      "|2024|    4|1878349.4199999988|\n",
      "|2023|    4|          60840.85|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_cols: list = []\n",
    "categorical_cols: list = []\n",
    "\n",
    "Transaction_schema = Transaction_Clean.schema\n",
    "# Separar colunas com base no tipo de dado\n",
    "for field in Transaction_schema:\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType, LongType)):\n",
    "        numeric_cols.append(field.name)\n",
    "    elif isinstance(field.dataType, (StringType, TimestampType)):\n",
    "        categorical_cols.append(field.name)\n",
    "\n",
    "numeric_cols.remove('Unnamed: 0')\n",
    "numeric_cols.remove(\"CustomerID\")\n",
    "Transaction_numerico = Transaction_Clean.select(numeric_cols)\n",
    "Transaction_categorico = Transaction_Clean.select(categorical_cols)"
   ],
   "id": "d21e2cb0942420a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T22:37:16.582576Z",
     "start_time": "2024-06-16T22:35:21.323691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calcula_outlier(dataframe: DataFrame, col_name: str) -> tuple:\n",
    "    q1 = dataframe.approxQuantile(col_name, [0.25], 0.01)[0]\n",
    "    q3 = dataframe.approxQuantile(col_name, [0.75], 0.01)[0]\n",
    "    iqr = q3 - q1\n",
    "    outliers_inferior = q1 - 1.5 * iqr\n",
    "    outliers_superior = q3 + 1.5 * iqr\n",
    "    return outliers_inferior, outliers_superior\n",
    "\n",
    "# Calcular os limites para cada coluna numérica\n",
    "outliers_limite = {col_name: calcula_outlier(Transaction_numerico, col_name) for col_name in numeric_cols}\n",
    "print(outliers_limite)"
   ],
   "id": "41d7159e19c4e602",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Quantity': (-3.0, 13.0), 'Price': (-34.145917880000006, 143.66259860000002), 'DiscountApplied(%)': (-9.8155356225, 29.6791015975), 'TotalAmount': (-299.96853877499996, 750.2632981449999)}\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T22:38:06.841935Z",
     "start_time": "2024-06-16T22:37:16.585596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Função para identificar outliers\n",
    "def identifica_outlier(dataframe: DataFrame, col_name: str, outliers_inferiores: float, outliers_superiores: float) -> DataFrame:\n",
    "    return dataframe.filter((col(col_name) < outliers_inferiores) | (col(col_name) > outliers_superiores))\n",
    "\n",
    "# Identificar e mostrar os outliers\n",
    "outliers = {}\n",
    "for col_name, (outlier_inferior, outlier_superior) in outliers_limite.items():\n",
    "    transaction_outlier = identifica_outlier(Transaction_numerico, col_name, outlier_inferior, outlier_superior)\n",
    "    outliers[col_name] = transaction_outlier\n",
    "\n",
    "# Mostrar os outliers para cada coluna\n",
    "for col_name, transaction_outlier in outliers.items():\n",
    "    print(f\"Outliers na coluna {col_name}:\")\n",
    "    transaction_outlier.show()"
   ],
   "id": "62113fdb6f52e474",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers na coluna Quantity:\n",
      "+--------+-----+------------------+-----------+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|\n",
      "+--------+-----+------------------+-----------+\n",
      "+--------+-----+------------------+-----------+\n",
      "\n",
      "Outliers na coluna Price:\n",
      "+--------+-----+------------------+-----------+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|\n",
      "+--------+-----+------------------+-----------+\n",
      "+--------+-----+------------------+-----------+\n",
      "\n",
      "Outliers na coluna DiscountApplied(%):\n",
      "+--------+-----+------------------+-----------+\n",
      "|Quantity|Price|DiscountApplied(%)|TotalAmount|\n",
      "+--------+-----+------------------+-----------+\n",
      "+--------+-----+------------------+-----------+\n",
      "\n",
      "Outliers na coluna TotalAmount:\n",
      "+--------+-----------+------------------+-----------+\n",
      "|Quantity|      Price|DiscountApplied(%)|TotalAmount|\n",
      "+--------+-----------+------------------+-----------+\n",
      "|       9| 90.3351513|       5.341062648|769.5926485|\n",
      "|       9|96.09278818|       3.435480217|835.1238551|\n",
      "|       9| 98.7028445|       1.266869772| 877.071672|\n",
      "|       9|90.36379105|       1.267585838|802.9651719|\n",
      "|       9|96.08480283|       13.16072454|750.9541195|\n",
      "|       9|95.59223377|       8.070660116|790.8957854|\n",
      "|       9|98.09163918|       11.33571314|782.7502711|\n",
      "|       9|96.94298506|       3.331673288|843.4184537|\n",
      "|       9|94.66163024|       2.077904086|834.2518712|\n",
      "|       9|91.16090501|       0.262245556|818.2965563|\n",
      "|       9|99.88844086|       1.850444087|  882.36055|\n",
      "|       9|90.10054469|       5.956831588|762.6006628|\n",
      "|       9|94.77635223|       7.220298815|791.3989475|\n",
      "|       8|98.95536366|       2.886172202|768.7947317|\n",
      "|       9|85.34599674|       0.524163412|764.0877982|\n",
      "|       9|93.76589606|       2.461751586|823.1185137|\n",
      "|       9|98.40126988|       13.94404586|762.1213651|\n",
      "|       9|98.07391721|       13.59411485|762.6747264|\n",
      "|       9|90.92989482|       0.579356056|813.6277827|\n",
      "|       9|98.83947846|       5.940162007|836.7142798|\n",
      "+--------+-----------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T18:52:50.017328Z",
     "start_time": "2024-06-16T18:52:25.083197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Transaction_media: list = [int(Transaction_Clean.select(mean(column).alias(\"Média\")).collect()[0]['Média'])for column in numeric_cols]\n",
    "Transaction_desvio_padrao: list = [int(Transaction_Clean.select(std(column).alias(\"Desvio Padrão\")).collect()[0][\"Desvio Padrão\"]) for column in numeric_cols]\n",
    "print(Transaction_media)\n",
    "print(Transaction_desvio_padrao)"
   ],
   "id": "bbb4ba0335718fda",
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2548.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 420.0 failed 1 times, most recent failure: Lost task 2.0 in stage 420.0 (TID 1617) (DESKTOP-1593J9O executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[157], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m Transaction_media: \u001B[38;5;28mlist\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTransaction_Clean\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMédia\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMédia\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnumeric_cols\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      2\u001B[0m Transaction_desvio_padrao: \u001B[38;5;28mlist\u001B[39m \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mint\u001B[39m(Transaction_Clean\u001B[38;5;241m.\u001B[39mselect(std(column)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDesvio Padrão\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDesvio Padrão\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m column \u001B[38;5;129;01min\u001B[39;00m numeric_cols]\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(Transaction_media)\n",
      "Cell \u001B[1;32mIn[157], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m Transaction_media: \u001B[38;5;28mlist\u001B[39m \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mint\u001B[39m(\u001B[43mTransaction_Clean\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMédia\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMédia\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;28;01mfor\u001B[39;00m column \u001B[38;5;129;01min\u001B[39;00m numeric_cols]\n\u001B[0;32m      2\u001B[0m Transaction_desvio_padrao: \u001B[38;5;28mlist\u001B[39m \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mint\u001B[39m(Transaction_Clean\u001B[38;5;241m.\u001B[39mselect(std(column)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDesvio Padrão\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDesvio Padrão\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m column \u001B[38;5;129;01min\u001B[39;00m numeric_cols]\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(Transaction_media)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Trabalho - Big Data\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1261\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1241\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[0;32m   1242\u001B[0m \n\u001B[0;32m   1243\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1258\u001B[0m \u001B[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001B[39;00m\n\u001B[0;32m   1259\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1260\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[1;32m-> 1261\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[1;32m~\\PycharmProjects\\Trabalho - Big Data\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\PycharmProjects\\Trabalho - Big Data\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Trabalho - Big Data\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o2548.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 420.0 failed 1 times, most recent failure: Lost task 2.0 in stage 420.0 (TID 1617) (DESKTOP-1593J9O executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "execution_count": 157
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
